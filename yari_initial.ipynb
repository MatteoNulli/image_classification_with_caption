{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davide = \"loser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import time\n",
    "import random \n",
    "import json\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"M1 pro GPU is activated\")\n",
    "    import os\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda_id = torch.cuda.current_device()\n",
    "    print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "        \n",
    "    print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the images already resized 250 x 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.csv') as fp:\n",
    "    # read a list of lines into data\n",
    "    data = fp.readlines()\n",
    "\n",
    "data[4790] = data[4790].replace(\"/\", \"\")\n",
    "data[14716] = data[14716].replace(\"/\", \"\")\n",
    "data[14961] = data[14961].replace(\"/\", \"\")\n",
    "data[29895] = data[29895].replace(\"/\", \"\")\n",
    "\n",
    "# and write everything back\n",
    "with open('train.csv', 'w') as file:\n",
    "    file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "FILENAME = 'train.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    df_train = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "FILENAME = 'test.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    df_test = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_train.shape[0]):\n",
    "    df_train.Labels[i] = [int(j) for j in df_train.Labels[i].split()]\n",
    "max_i = 0\n",
    "for i in df_train.Labels:\n",
    "    max_i = max(max_i, max(i))\n",
    "min_i = 19\n",
    "for i in df_train.Labels:\n",
    "    min_i = min(min_i, min(i))\n",
    "min_i\n",
    "\n",
    "for i in range(1, max_i+1):\n",
    "    df_train[f'{i}'] = 0\n",
    "for i in range(df_train.shape[0]):\n",
    "    for j in df_train.Labels[i]:\n",
    "        df_train[f\"{j}\"][i] = 1\n",
    "df_train.to_csv(\"df_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_train.shape[0]):\n",
    "    if f\"{i}.jpg\" != df_train.loc[i,'ImageID']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['Labels'].explode()\n",
    "\n",
    "# Plot the distribution of labels using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts = labels.value_counts()\n",
    "label_counts.plot(kind='bar')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['Labels'].explode()\n",
    "\n",
    "# Get the label counts\n",
    "label_counts = labels.value_counts().sort_index()\n",
    "\n",
    "# Add missing label if it doesn't exist\n",
    "if '12' not in label_counts.index:\n",
    "    label_counts['12'] = 0\n",
    "\n",
    "# Sort labels based on their values\n",
    "label_counts = label_counts.sort_values()\n",
    "\n",
    "# Plot the distribution of labels using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['12'], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv, train = True, test = False,full=False):\n",
    "        self.csv = csv # df_train\n",
    "        self.train = train # boolean\n",
    "        self.full = full\n",
    "        self.test = test # boolean\n",
    "\n",
    "        self.all_image_names = self.csv[:]['ImageID']\n",
    "        self.captions = self.csv[:]['Caption']\n",
    "\n",
    "        #self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "\n",
    "        self.train_ratio = int(0.85 * len(self.csv))\n",
    "        self.valid_ratio = len(self.csv) - self.train_ratio\n",
    "\n",
    "        # set the training data images and labels\n",
    "        if self.full == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "            self.image_names = list(self.all_image_names)\n",
    "            self.labels = list(self.all_labels)\n",
    "            print(f\"Number of training images: {self.all_labels.shape[0]}\")\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                # transforms.ColorJitter(),\n",
    "                transforms.Normalize(mean, std)\n",
    "\n",
    "            ])\n",
    "        elif self.train == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "            print(f\"Number of training images: {self.train_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[:self.train_ratio])\n",
    "            self.labels = list(self.all_labels[:self.train_ratio])\n",
    "            self.captions = list(self.captions[:self.train_ratio])\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                # transforms.ColorJitter(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        # set the validation data images and labels\n",
    "        elif self.train == False and self.test == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels','Caption'], axis=1))\n",
    "            print(f\"Number of validation images: {self.valid_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[-self.valid_ratio:])\n",
    "            self.labels = list(self.all_labels[-self.valid_ratio:])\n",
    "            self.captions = list(self.captions[-self.valid_ratio:])\n",
    "            # define the validation transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.image_names[index]\n",
    "        image = Image.open(f'data/{name}')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[index]\n",
    "        if self.train == True:   #training\n",
    "            targets = self.labels[index]\n",
    "            return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'label': torch.tensor(targets, dtype=torch.float32),\n",
    "            'name' : name,\n",
    "            'caption' : caption,\n",
    "        }\n",
    "        elif self.train == False and self.test == True:  #validation\n",
    "             targets = self.labels[index]\n",
    "             return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'label': torch.tensor(targets, dtype=torch.float32),\n",
    "            'caption' : caption,\n",
    "            'name' : name,\n",
    "        }\n",
    "        elif self.test == True and self.train == False:   #testing\n",
    "            return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'name' : name,\n",
    "            'caption' : caption,\n",
    "        }\n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv\n",
    "        self.image_names = self.csv[:]['ImageID']\n",
    "        self.captions = self.csv[:]['Caption']\n",
    "        print(f\"Number of test images: {len(self.csv)}\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "            \n",
    "        ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.image_names[index]\n",
    "        image = Image.open(f'data/{name}')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[index]\n",
    "        return {\n",
    "        'image': torch.tensor(image, dtype=torch.float32),\n",
    "        'name' : name,\n",
    "        'caption' : caption,\n",
    "        }\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = TrainDataset(\n",
    "    df_train, train=True, test=False\n",
    ")\n",
    "# validation dataset\n",
    "valid_data = TrainDataset(\n",
    "    df_train, train=False, test=True\n",
    ")\n",
    "\n",
    "full_data = TrainDataset(\n",
    "   csv = df_train, full=True\n",
    ")\n",
    "\n",
    "test_data = TestDataset(\n",
    "    csv = df_test\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "# validation data loader\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "full_loader = DataLoader(\n",
    "    full_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0]['caption'], valid_data[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_data[0]['image'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(valid_data[0]['image'].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]['caption'], train_data[0]['name'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[0]['image'].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[len(test_data) -1]['caption'], test_data[len(test_data) -1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[len(test_data) -1]['image'].permute(1, 2, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for predicting the labels on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_vision(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            #images\n",
    "            batch_size = len(data['image'])\n",
    "            images = data['image'].to(device)\n",
    "            pred = model(images)\n",
    "            _, argmax_indices = torch.max(pred, dim=1)\n",
    "            pred[torch.arange(pred.size(0)), argmax_indices] = 1\n",
    "            pred = (pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1\n",
    "            # print(indices)\n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_caption(model, test_loader, device, tokenizer, seq_len, word_index):\n",
    "    model.eval()\n",
    "    y_pred = torch.zeros(len(test_data), 18)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_size = len(data['caption'])\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "            #captions = data['Caption'].to(device)\n",
    "            pred = model(captions)\n",
    "            _, argmax_indices = torch.max(pred, dim=1)\n",
    "            pred[torch.arange(pred.size(0)), argmax_indices] = 1\n",
    "            pred = (pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1 ### incrementing labels that are bigger than 10, since we eliminated the 12 label\n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_combined(best_cv_model, best_nlp_model, classifier, test_loader, device, tokenizer, seq_len, word_index):\n",
    "    best_cv_model.eval()\n",
    "    best_nlp_model.eval()\n",
    "    classifier.eval()\n",
    "    y_pred = torch.zeros(len(test_data), 18)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_size = len(data['caption'])\n",
    "            print(batch_size)\n",
    "            #captions\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "            #images\n",
    "            images = data['image'].to(device)\n",
    "\n",
    "            #captions = data['Caption'].to(device)\n",
    "            #optimizer.zero_grad()\n",
    "            img_out = best_cv_model(images)\n",
    "            nlp_out = best_nlp_model(captions)\n",
    "            concatenating_outs = torch.concat((img_out, nlp_out), 1)\n",
    "            combined_model_pred = classifier(concatenating_outs)\n",
    "            _, argmax_indices = torch.max(combined_model_pred, dim=1)\n",
    "            combined_model_pred[torch.arange(combined_model_pred.size(0)), argmax_indices] = 1\n",
    "            combined_model_pred = (combined_model_pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = combined_model_pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1 ### incrementing labels that are bigger than 10, since we eliminated the 12 label       \n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "class AlexNet_1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        alexnet = AlexNet_model.to(device)\n",
    "        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features,num_classes)\n",
    "        self.base_model = alexnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                \n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "class AlexNet_2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        alexnet = AlexNet_model.to(device)\n",
    "        alexnet.classifier[4] = nn.Linear(alexnet.classifier[4].in_features,1096)\n",
    "        alexnet.classifier[6] = nn.Linear(1096,num_classes)\n",
    "        self.base_model = alexnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                \n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True).to(device=device)\n",
    "\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True).to(device=device)\n",
    "        resnet.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Resnet18(\n",
       "  (base_model): ResNet(\n",
       "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (relu): ReLU(inplace=True)\n",
       "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (downsample): Sequential(\n",
       "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        )\n",
       "      )\n",
       "      (1): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU(inplace=True)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "    (fc): Linear(in_features=512, out_features=18, bias=True)\n",
       "  )\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_cv_model = Resnet18(num_classes=18)\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_cv_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_freq = 3\n",
    "lr = 1e-4\n",
    "weight_decay = 1e-5\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(best_cv_model.parameters(),weight_decay=weight_decay, lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "name_of_model = f'Resnet18_lr={lr}_weight_decay={weight_decay}_dropout=0.5_original_model'\n",
    "os.mkdir(name_of_model)\n",
    "train_losses = np.zeros(num_epochs)\n",
    "valid_losses = np.zeros(num_epochs)\n",
    "f1_scores = np.zeros(num_epochs)\n",
    "initial_lr = optimizer.param_groups[0]['lr']\n",
    "for it in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images = data['image'].to(device)\n",
    "        target = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_cv_model(images)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(it)\n",
    "\n",
    "    if ((it % save_freq == 0) or (it == num_epochs - 1)) and it > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{it}.pth'\n",
    "        checkpoint = {'model': best_cv_model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    train_losses[it] = train_loss/len(train_loader)\n",
    "    result = best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    valid_losses[it] = result['loss']\n",
    "    f1_scores[it] = result['f1_score']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.78817068, 0.77770771, 0.78325996, 0.79319001, 0.8028692 ,\n",
       "       0.79498366, 0.79609065, 0.80616267, 0.79312722, 0.8110853 ,\n",
       "       0.8053035 , 0.80309238, 0.80375572, 0.79297839, 0.79091517,\n",
       "       0.79404372, 0.7954371 , 0.79290042, 0.78982501, 0.79113428])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8110852974186308"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb Cell 46\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb#Y323sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m best_cv_model\u001b[39m.\u001b[39;49mcalculate_f1_score_and_loss(valid_loader, criterion\u001b[39m=\u001b[39;49mcriterion, device\u001b[39m=\u001b[39;49mdevice)\n",
      "\u001b[1;32m/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb Cell 46\u001b[0m in \u001b[0;36mResnet18.calculate_f1_score_and_loss\u001b[0;34m(self, loader, criterion, device)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb#Y323sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(images)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb#Y323sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m loss \u001b[39m=\u001b[39m criterion(outputs, target)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb#Y323sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m val_loss[i] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39;49mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb#Y323sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m outputs \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mcpu()\u001b[39m.\u001b[39mnumpy()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/yaraslauivashynka/Desktop/projects/Dl_assignment2/image_classification_with_caption/yari_initial.ipynb#Y323sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m argmax_indices \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(outputs, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = predict_test_data_vision(best_cv_model, test_loader=test_loader, device=device, batch_size = batch_size)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),valid_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{name_of_model}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "    \"f1_scores\": f1_scores.tolist(),\n",
    "    \"train_losses\": train_losses.tolist(),\n",
    "    \"valid_losses\": valid_losses.tolist()\n",
    "}\n",
    "  \n",
    "# the json file where the output must be stored\n",
    "out_file = open(f\"{name_of_model}/myfile.json\", \"w\")\n",
    "\n",
    "\n",
    "json.dump(dict1, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_model = AlexNet_2(18)\n",
    "checkpoint = torch.load(\"AlexNet_lr=0.0001_weight_decay=0_dropout=0.5/AlexNet_lr=0.0001_weight_decay=0_dropout=0.519.pth\")\n",
    "best_cv_model.load_state_dict(checkpoint['model'])\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Woman in swim suit holding parasol on sunny day.\n",
       "1        A couple of men riding horses on top of a gree...\n",
       "2        They are brave for riding in the jungle on tho...\n",
       "3        a black and silver clock tower at an intersect...\n",
       "4         A train coming to a stop on the tracks out side.\n",
       "                               ...                        \n",
       "39995    A group of men riding surfboards riding a mass...\n",
       "39996    A motorcycle parked next to a car in a parking...\n",
       "39997              a little boy that is playing with a wii\n",
       "39998    group of kids play Frisbee golf in the middle ...\n",
       "39999     A man in a gray jacket standing next to a woman.\n",
       "Name: Caption, Length: 40000, dtype: object"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = df_train.drop(columns = 'Caption').join(df_train['Caption'].str.replace('\\\"', ''))\n",
    "df_test = df_test.drop(columns = 'Caption').join(df_test['Caption'].str.replace('\\\"', ''))\n",
    "whole_sentences = pd.concat([df_train['Caption'], df_test['Caption']], axis=0, ignore_index=True)\n",
    "whole_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/yaraslauivashynka/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/yaraslauivashynka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yaraslauivashynka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "# stop_words = sw.words()\n",
    "STOPWORDS = set(sw.words('english'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split(input_string): # string to pure word\n",
    "    splits = []\n",
    "    for sent in input_string:\n",
    "        # print(text)\n",
    "        sent = sent.lower() # lowercase\n",
    "        sent = re.sub(r'[^A-Za-z]+', ' ', sent) # remove symbols / digits\n",
    "        # text = re.sub(r'[0-9]','',text)\n",
    "        orig_sent = []\n",
    "        for item in sent.split():\n",
    "            if item not in STOPWORDS: # remove stopword\n",
    "                orig_sent.append(item)\n",
    "\n",
    "        lem = [lemmatizer.lemmatize(sent) for sent in orig_sent] # lammatisation\n",
    "\n",
    "        # token = [word_tokenize(word) for word in text_le]\n",
    "        splits.append(lem)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_seq_len = max(len(s) for s in sent_split(whole_sentences))\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "word_embedding_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "233353"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splitted_sentences = sent_split(whole_sentences)\n",
    "word_list = []\n",
    "for sent in splitted_sentences:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for sent in splitted_sentences:\n",
    "    for word in sent:\n",
    "        vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.          0.          0.         ...  0.          0.\n",
      "   0.        ]\n",
      " [ 0.17480469  0.17871094  0.09082031 ...  0.07568359 -0.11181641\n",
      "  -0.0625    ]\n",
      " ...\n",
      " [-0.08544922 -0.10253906 -0.48632812 ...  0.02075195  0.08496094\n",
      "   0.00061035]\n",
      " [-0.07421875 -0.10205078  0.20117188 ... -0.25390625  0.06054688\n",
      "  -0.21289062]\n",
      " [-0.04248047  0.16015625 -0.2265625  ...  0.18945312 -0.00692749\n",
      "   0.1328125 ]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2218, 300)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the Embedding lookup table\n",
    "import numpy as np\n",
    "emb_dim = word_embedding_model.vector_size\n",
    "word_set = set()\n",
    "min_freq = 5\n",
    "from collections import Counter\n",
    "c = Counter(vocab_list)\n",
    "for i in c:\n",
    "    if c[i] >= min_freq:\n",
    "        word_set.add(i)\n",
    "word_set.add('[PAD]')\n",
    "word_set.add('[UNKOWN]')\n",
    "word_list=list(word_set)\n",
    "word_list.sort()\n",
    "emb_table = []\n",
    "word_index = {}\n",
    "emb_table = []\n",
    "for i, word in enumerate(word_list):\n",
    "    word_index[word] = i\n",
    "    if word in word_embedding_model:\n",
    "        emb_table.append(word_embedding_model[word])\n",
    "    else:\n",
    "        emb_table.append([0]*emb_dim)\n",
    "emb_table = np.array(emb_table)\n",
    "    \n",
    "print(emb_table)\n",
    "emb_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tokenize(self, sentences, seq_len, word_index):\n",
    "        sentences = sent_split(sentences)\n",
    "        sent_encoded = []\n",
    "        for sent in sentences:\n",
    "            temp_encoded = [word_index[word] if word in word_index else word_index['[UNKOWN]'] for word in sent]\n",
    "            if len(temp_encoded) < seq_len:\n",
    "                temp_encoded += [word_index['[PAD]']] * (seq_len - len(temp_encoded))\n",
    "            else:\n",
    "                temp_encoded = temp_encoded[:seq_len]\n",
    "            sent_encoded.append(temp_encoded)\n",
    "        return sent_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2218, 300)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = emb_table.shape[0]\n",
    "emb_dim = emb_table.shape[1]\n",
    "vocab_size, emb_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "n_hidden = 300\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "seq_len = max_seq_len\n",
    "\n",
    "class Bi_LSTM_Emb(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Bi_LSTM_Emb, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
    "        self.emb.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n",
    "        self.linear = nn.Linear(n_hidden*2, n_classes)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.emb(x)        \n",
    "        lstm_out, (h_n,c_n) = self.lstm(x)\n",
    "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
    "        z = self.linear(hidden_out)\n",
    "        return self.sigm(z)\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device):\n",
    "            self.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for data in loader:\n",
    "                    captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "                    captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "                    target = data['label'].to(device)\n",
    "                    outputs = self(captions)\n",
    "                    loss = criterion(outputs, target)\n",
    "                    val_loss += loss.item()\n",
    "                    outputs = outputs.cpu().numpy()\n",
    "                    argmax_indices = np.argmax(outputs, axis=1)\n",
    "                    outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                    predicted = np.round(outputs)\n",
    "                    y_pred.extend(predicted)\n",
    "                    y_true.extend(target.cpu().numpy())\n",
    "                y_pred = np.array(y_pred)\n",
    "                y_true = np.array(y_true)\n",
    "                res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "            return {'f1_score' : res, 'loss' : val_loss/len(loader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "n_hidden = 300\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "seq_len = max_seq_len\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, n_class):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "    self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
    "    self.emb.weight.requires_grad = False\n",
    "    self.lstm = nn.LSTM(emb_dim, n_hidden, num_layers=2, batch_first =True, dropout=0.5)\n",
    "    self.linear = nn.Linear(n_hidden,n_class)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "  \n",
    "  def forward(self, input):\n",
    "    input = self.emb(input)\n",
    "    input,_ = self.lstm(input)\n",
    "    input = self.linear(input[:,-1,:])\n",
    "    return self.sigmoid(input)\n",
    "  \n",
    "  def calculate_f1_score_and_loss(self, loader, criterion, device):\n",
    "    self.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for data in loader:\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "            target = data['label'].to(device)\n",
    "            outputs = self(captions)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            argmax_indices = np.argmax(outputs, axis=1)\n",
    "            outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "            predicted = np.round(outputs)\n",
    "            y_pred.extend(predicted)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = np.array(y_true)\n",
    "        res = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    return {'f1_score' : res, 'loss' : val_loss/len(loader)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Bi_LSTM_Emb(\n",
       "  (emb): Embedding(2218, 300)\n",
       "  (lstm): LSTM(300, 300, batch_first=True, bidirectional=True)\n",
       "  (linear): Linear(in_features=600, out_features=18, bias=True)\n",
       "  (sigm): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_nlp_model = Bi_LSTM_Emb(18)\n",
    "best_nlp_model = best_nlp_model.to(device)\n",
    "best_nlp_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "save_freq = 3\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(best_nlp_model.parameters(), weight_decay=weight_decay, lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_model = f'BiLSTM_lr={lr}_weight_decay={weight_decay}_dropout=0.5'\n",
    "os.mkdir(name_of_model)\n",
    "train_losses = torch.zeros(num_epochs)\n",
    "val_losses = torch.zeros(num_epochs)\n",
    "f1_scores = torch.zeros(num_epochs)\n",
    "for it in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    best_nlp_model.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "        captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "        target = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_nlp_model(captions)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_loss+=loss.item()\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if ((it % save_freq == 0) or (it == num_epochs - 1)) and it > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{it}.pth'\n",
    "        checkpoint = {'model': best_cv_model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)        \n",
    "    print(it)\n",
    "    train_losses[it] = train_loss/len(train_loader)\n",
    "    result = best_nlp_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    val_losses[it] = result['loss']\n",
    "    f1_scores[it] = result['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2237, 0.2004, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005,\n",
       "        0.2006, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2005, 0.2006,\n",
       "        0.2005, 0.2005])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),valid_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{name_of_model}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "    \"f1_scores\": f1_scores.tolist(),\n",
    "    \"train_losses\": train_losses.tolist(),\n",
    "    \"valid_losses\": valid_losses.tolist()\n",
    "}\n",
    "  \n",
    "# the json file where the output must be stored\n",
    "out_file = open(f\"{name_of_model}/myfile.json\", \"w\")\n",
    "json.dump(dict1, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINED MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine the NLP and classification model we need to concatenate the features and run a third classifier on this concatenated features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the third classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_f1_score_and_loss(model, best_nlp_model, best_cv_model, loader, criterion, seq_len, word_index, tokenizer, device=None):\n",
    "        model.eval()\n",
    "        best_nlp_model.eval()\n",
    "        best_cv_model.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "\n",
    "                captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "                captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "\n",
    "                image_outs = best_cv_model(images)\n",
    "                text_outs = best_nlp_model(captions)\n",
    "\n",
    "                concatenating_outs = torch.concat((image_outs, text_outs), 1)\n",
    "                combined_outs = classifier(concatenating_outs)\n",
    "\n",
    "                loss = criterion(combined_outs, targets.type(torch.float))\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = combined_outs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 18\n",
    "\n",
    "best_cv_model = Resnext50(num_classes=n_labels)\n",
    "best_nlp_model = Bi_LSTM_Emb(n_classes=n_labels)\n",
    "checkpoint = torch.load(\"ResNet4.pth\")\n",
    "# Load the model state dictionary from the checkpoint\n",
    "best_cv_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "checkpoint = torch.load(\"BiLSTM3.pth\")\n",
    "# Load the model state dictionary from the checkpoint\n",
    "best_nlp_model.load_state_dict(checkpoint['model'])\n",
    "# Define Classifier\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=36, out_features=3100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=3100, out_features=n_labels),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "# Switch model to GPU.\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_nlp_model = best_nlp_model.to(device)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device=device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_losses = torch.zeros(num_epochs)\n",
    "val_losses = torch.zeros(num_epochs)\n",
    "f1_scores = torch.zeros(num_epochs)\n",
    "name_of_model = \"combined_model\"\n",
    "saving_freq = 3\n",
    "os.mkdir(name_of_model)\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    batch_loss_values = []\n",
    "    train_loss=0\n",
    "    for data in train_loader:\n",
    "        best_cv_model.train()\n",
    "        best_nlp_model.train()\n",
    "\n",
    "        targets = data['label'].to(device)\n",
    "        images = data['image'].to(device)\n",
    "       \n",
    "        captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "        captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_out = best_cv_model(images)\n",
    "        nlp_out = best_nlp_model(captions)\n",
    "        concatenating_outs = torch.concat((img_out, nlp_out), 1)\n",
    "\n",
    "        combined_model = classifier(concatenating_outs)\n",
    "        \n",
    "        #loss and backward pass\n",
    "        loss = criterion(combined_model, targets.type(torch.float))\n",
    "        train_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "        # bl_value = loss.item()\n",
    "        # print(bl_value)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "    \n",
    "    if ((epoch % saving_freq == 0) or (epoch == epochs - 1)) and epoch > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{epoch}.pth'\n",
    "        checkpoint = {'model': classifier.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "\n",
    "    print(f\"epoch {epoch}\")\n",
    "    train_losses[it] = train_loss/len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),val_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
