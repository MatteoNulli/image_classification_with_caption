{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "davide = \"loser\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms as T\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.optim as optim\n",
    "from torchvision import models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, Subset\n",
    "import time\n",
    "import random \n",
    "import json\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.backends.mps.is_built():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"M1 pro GPU is activated\")\n",
    "    import os\n",
    "    os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '0'\n",
    "else:\n",
    "    device = torch.device(\"cuda\")\n",
    "    cuda_id = torch.cuda.current_device()\n",
    "    print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "        \n",
    "    print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will import the images already resized 250 x 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('train.csv') as fp:\n",
    "    # read a list of lines into data\n",
    "    data = fp.readlines()\n",
    "\n",
    "data[4790] = data[4790].replace(\"/\", \"\")\n",
    "data[14716] = data[14716].replace(\"/\", \"\")\n",
    "data[14961] = data[14961].replace(\"/\", \"\")\n",
    "data[29895] = data[29895].replace(\"/\", \"\")\n",
    "\n",
    "# and write everything back\n",
    "with open('train.csv', 'w') as file:\n",
    "    file.writelines(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "FILENAME = 'train.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    df_train = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "FILENAME = 'test.csv'\n",
    "with open(FILENAME) as file:\n",
    "    lines = [re.sub(r'([^,])\"(\\s*[^\\n])', r'\\1/\"\\2', line) for line in file]\n",
    "    df_test = pd.read_csv(StringIO(''.join(lines)), escapechar=\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_train.shape[0]):\n",
    "    df_train.Labels[i] = [int(j) for j in df_train.Labels[i].split()]\n",
    "max_i = 0\n",
    "for i in df_train.Labels:\n",
    "    max_i = max(max_i, max(i))\n",
    "min_i = 19\n",
    "for i in df_train.Labels:\n",
    "    min_i = min(min_i, min(i))\n",
    "min_i\n",
    "\n",
    "for i in range(1, max_i+1):\n",
    "    df_train[f'{i}'] = 0\n",
    "for i in range(df_train.shape[0]):\n",
    "    for j in df_train.Labels[i]:\n",
    "        df_train[f\"{j}\"][i] = 1\n",
    "df_train.to_csv(\"df_train.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(df_train.shape[0]):\n",
    "    if f\"{i}.jpg\" != df_train.loc[i,'ImageID']:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"df_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distribution of Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['Labels'].explode()\n",
    "\n",
    "# Plot the distribution of labels using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts = labels.value_counts()\n",
    "label_counts.plot(kind='bar')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = df_train['Labels'].explode()\n",
    "\n",
    "# Get the label counts\n",
    "label_counts = labels.value_counts().sort_index()\n",
    "\n",
    "# Add missing label if it doesn't exist\n",
    "if '12' not in label_counts.index:\n",
    "    label_counts['12'] = 0\n",
    "\n",
    "# Sort labels based on their values\n",
    "label_counts = label_counts.sort_values()\n",
    "\n",
    "# Plot the distribution of labels using a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "label_counts.plot(kind='bar')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Labels')\n",
    "\n",
    "# Set custom x-tick labels\n",
    "plt.xticks(range(len(label_counts)), label_counts.index, rotation=45)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(['12'], axis=1)\n",
    "df_train.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "class TrainDataset(Dataset):\n",
    "    def __init__(self, csv, train = True, test = False,full=False):\n",
    "        self.csv = csv # df_train\n",
    "        self.train = train # boolean\n",
    "        self.full = full\n",
    "        self.test = test # boolean\n",
    "\n",
    "        self.all_image_names = self.csv[:]['ImageID']\n",
    "        self.captions = self.csv[:]['Caption']\n",
    "\n",
    "        #self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "\n",
    "        self.train_ratio = int(0.85 * len(self.csv))\n",
    "        self.valid_ratio = len(self.csv) - self.train_ratio\n",
    "\n",
    "        # set the training data images and labels\n",
    "        if self.full == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "            self.image_names = list(self.all_image_names)\n",
    "            self.labels = list(self.all_labels)\n",
    "            print(f\"Number of training images: {self.all_labels.shape[0]}\")\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                # transforms.ColorJitter(),\n",
    "                transforms.Normalize(mean, std)\n",
    "\n",
    "            ])\n",
    "        elif self.train == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels', 'Caption'], axis=1))\n",
    "            print(f\"Number of training images: {self.train_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[:self.train_ratio])\n",
    "            self.labels = list(self.all_labels[:self.train_ratio])\n",
    "            self.captions = list(self.captions[:self.train_ratio])\n",
    "            # define the training transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomRotation(degrees=45),\n",
    "                # transforms.ColorJitter(),\n",
    "                transforms.Normalize(mean, std)\n",
    "            ])\n",
    "        # set the validation data images and labels\n",
    "        elif self.train == False and self.test == True:\n",
    "            self.all_labels = np.array(self.csv.drop(['ImageID', 'Labels','Caption'], axis=1))\n",
    "            print(f\"Number of validation images: {self.valid_ratio}\")\n",
    "            self.image_names = list(self.all_image_names[-self.valid_ratio:])\n",
    "            self.labels = list(self.all_labels[-self.valid_ratio:])\n",
    "            self.captions = list(self.captions[-self.valid_ratio:])\n",
    "            # define the validation transforms\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.Resize((256, 256)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(mean, std)\n",
    "\n",
    "            ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.image_names[index]\n",
    "        image = Image.open(f'data/{name}')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[index]\n",
    "        if self.train == True:   #training\n",
    "            targets = self.labels[index]\n",
    "            return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'label': torch.tensor(targets, dtype=torch.float32),\n",
    "            'name' : name,\n",
    "            'caption' : caption,\n",
    "        }\n",
    "        elif self.train == False and self.test == True:  #validation\n",
    "             targets = self.labels[index]\n",
    "             return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'label': torch.tensor(targets, dtype=torch.float32),\n",
    "            'caption' : caption,\n",
    "            'name' : name,\n",
    "        }\n",
    "        elif self.test == True and self.train == False:   #testing\n",
    "            return {\n",
    "            'image': torch.tensor(image, dtype=torch.float32),\n",
    "            'name' : name,\n",
    "            'caption' : caption,\n",
    "        }\n",
    "       \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(Dataset):\n",
    "    def __init__(self, csv):\n",
    "        self.csv = csv\n",
    "        self.image_names = self.csv[:]['ImageID']\n",
    "        self.captions = self.csv[:]['Caption']\n",
    "        print(f\"Number of test images: {len(self.csv)}\")\n",
    "        self.transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize(mean, std)\n",
    "            \n",
    "        ])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.csv)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        name = self.image_names[index]\n",
    "        image = Image.open(f'data/{name}')\n",
    "        image = self.transform(image)\n",
    "        caption = self.captions[index]\n",
    "        return {\n",
    "        'image': torch.tensor(image, dtype=torch.float32),\n",
    "        'name' : name,\n",
    "        'caption' : caption,\n",
    "        }\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_data = TrainDataset(\n",
    "    df_train, train=True, test=False\n",
    ")\n",
    "# validation dataset\n",
    "valid_data = TrainDataset(\n",
    "    df_train, train=False, test=True\n",
    ")\n",
    "\n",
    "full_data = TrainDataset(\n",
    "   csv = df_train, full=True\n",
    ")\n",
    "\n",
    "test_data = TestDataset(\n",
    "    csv = df_test\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")\n",
    "# validation data loader\n",
    "valid_loader = DataLoader(\n",
    "    valid_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "full_loader = DataLoader(\n",
    "    full_data,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data[0]['caption'], valid_data[0]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(valid_data[0]['image'].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(valid_data[0]['image'].permute(1, 2, 0))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]['caption'], train_data[0]['name'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(train_data[0]['image'].permute(1, 2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data[len(test_data) -1]['caption'], test_data[len(test_data) -1]['name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(test_data[len(test_data) -1]['image'].permute(1, 2, 0))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for predicting the labels on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_vision(model, test_loader, device):\n",
    "    model.eval()\n",
    "    model = model.to(device)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            #images\n",
    "            batch_size = len(data['image'])\n",
    "            images = data['image'].to(device)\n",
    "            pred = model(images)\n",
    "            _, argmax_indices = torch.max(pred, dim=1)\n",
    "            pred[torch.arange(pred.size(0)), argmax_indices] = 1\n",
    "            pred = (pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1\n",
    "            # print(indices)\n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_caption(model, test_loader, device, tokenizer, seq_len, word_index):\n",
    "    model.eval()\n",
    "    y_pred = torch.zeros(len(test_data), 18)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_size = len(data['caption'])\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "            #captions = data['Caption'].to(device)\n",
    "            pred = model(captions)\n",
    "            _, argmax_indices = torch.max(pred, dim=1)\n",
    "            pred[torch.arange(pred.size(0)), argmax_indices] = 1\n",
    "            pred = (pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1 ### incrementing labels that are bigger than 10, since we eliminated the 12 label\n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_test_data_combined(best_cv_model, best_nlp_model, classifier, test_loader, device, tokenizer, seq_len, word_index):\n",
    "    best_cv_model.eval()\n",
    "    best_nlp_model.eval()\n",
    "    classifier.eval()\n",
    "    y_pred = torch.zeros(len(test_data), 18)\n",
    "    df_pred = pd.DataFrame()\n",
    "    df_pred['ImageID'] = [\"\" for i in range(len(test_data))]\n",
    "    df_pred['Labels'] = [\"\" for i in range(len(test_data))]\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(test_loader):\n",
    "            batch_size = len(data['caption'])\n",
    "            print(batch_size)\n",
    "            #captions\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "            #images\n",
    "            images = data['image'].to(device)\n",
    "\n",
    "            #captions = data['Caption'].to(device)\n",
    "            #optimizer.zero_grad()\n",
    "            img_out = best_cv_model(images)\n",
    "            nlp_out = best_nlp_model(captions)\n",
    "            concatenating_outs = torch.concat((img_out, nlp_out), 1)\n",
    "            combined_model_pred = classifier(concatenating_outs)\n",
    "            _, argmax_indices = torch.max(combined_model_pred, dim=1)\n",
    "            combined_model_pred[torch.arange(combined_model_pred.size(0)), argmax_indices] = 1\n",
    "            combined_model_pred = (combined_model_pred >= 0.5).type(torch.uint8)\n",
    "            start = i*batch_size\n",
    "            end = min((i+1)*batch_size, len(test_data))\n",
    "            indices = combined_model_pred.nonzero()\n",
    "            indices[indices[:, 1] > 10, 1] += 1 ### incrementing labels that are bigger than 10, since we eliminated the 12 label       \n",
    "            indices_by_row = torch.split(indices[:, 1], indices[:, 0].unique(return_counts=True)[1].tolist())\n",
    "            results = [\" \".join([str(idx.item()+1) for idx in row]) for row in indices_by_row]\n",
    "            df_pred[\"ImageID\"][start:end] = data['name']\n",
    "            df_pred[\"Labels\"][start:end] = results\n",
    "\n",
    "    return df_pred"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computer Vision Models\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "class AlexNet_1(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        alexnet = AlexNet_model.to(device)\n",
    "        alexnet.classifier[6] = nn.Linear(alexnet.classifier[6].in_features,num_classes)\n",
    "        self.base_model = alexnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                \n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AlexNet_model = torch.hub.load('pytorch/vision:v0.6.0', 'alexnet', pretrained=True)\n",
    "class AlexNet_2(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        alexnet = AlexNet_model.to(device)\n",
    "        alexnet.classifier[4] = nn.Linear(alexnet.classifier[4].in_features,1096)\n",
    "        alexnet.classifier[6] = nn.Linear(1096,num_classes)\n",
    "        self.base_model = alexnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                \n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resnext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Use the torchvision's implementation of ResNeXt, but add FC layer for a different number of classes (27) and a Sigmoid instead of a default Softmax.\n",
    "class Resnext50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnext50_32x4d(pretrained=True).to(device=device)\n",
    "\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Resnet18(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        resnet = models.resnet18(pretrained=True).to(device=device)\n",
    "        resnet.fc = nn.Linear(in_features=512, out_features=num_classes, bias=True)\n",
    "        self.base_model = resnet\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.sigm(self.base_model(x))\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device=None):\n",
    "        self.eval()\n",
    "        val_loss = torch.zeros(len(loader))\n",
    "        with torch.no_grad():\n",
    "            y_pred = []\n",
    "            y_true = []\n",
    "            for i, data in enumerate(loader):\n",
    "                images = data['image'].to(device)\n",
    "                target = data['label'].to(device)\n",
    "                outputs = self(images).to(device)\n",
    "                loss = criterion(outputs, target)\n",
    "                val_loss[i] = loss.item()\n",
    "                outputs = outputs.cpu().numpy()\n",
    "                argmax_indices = np.argmax(outputs, axis=1)\n",
    "                outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                predicted = np.round(outputs)\n",
    "                y_pred.extend(predicted)\n",
    "                y_true.extend(target.cpu().numpy())\n",
    "            y_pred = np.array(y_pred)\n",
    "            y_true = np.array(y_true)\n",
    "            res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "        return {'f1_score' : res, 'loss' : val_loss.mean()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_model = Resnet18(num_classes=18)\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_cv_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_freq = 3\n",
    "lr = 1e-4\n",
    "weight_decay = 0\n",
    "num_epochs = 20\n",
    "optimizer = torch.optim.Adam(best_cv_model.parameters(),weight_decay=weight_decay, lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:86: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n",
      "/var/folders/yz/zpp3kyv15wgc4kxcns1r1_v80000gn/T/ipykernel_58729/653025415.py:78: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'image': torch.tensor(image, dtype=torch.float32),\n"
     ]
    }
   ],
   "source": [
    "name_of_model = f'Resnet18_lr={lr}_weight_decay={weight_decay}_dropout=0.5_original_model'\n",
    "os.mkdir(name_of_model)\n",
    "train_losses = np.zeros(num_epochs)\n",
    "valid_losses = np.zeros(num_epochs)\n",
    "f1_scores = np.zeros(num_epochs)\n",
    "initial_lr = optimizer.param_groups[0]['lr']\n",
    "for it in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        images = data['image'].to(device)\n",
    "        target = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_cv_model(images)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(it)\n",
    "\n",
    "    if ((it % save_freq == 0) or (it == num_epochs - 1)) and it > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{it}.pth'\n",
    "        checkpoint = {'model': best_cv_model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)\n",
    "    train_losses[it] = train_loss/len(train_loader)\n",
    "    result = best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    valid_losses[it] = result['loss']\n",
    "    f1_scores[it] = result['f1_score']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, valid_losses, f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(f1_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = predict_test_data_vision(best_cv_model, test_loader=test_loader, device=device, batch_size = batch_size)\n",
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),valid_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{name_of_model}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "    \"f1_scores\": f1_scores.tolist(),\n",
    "    \"train_losses\": train_losses.tolist(),\n",
    "    \"valid_losses\": valid_losses.tolist()\n",
    "}\n",
    "  \n",
    "# the json file where the output must be stored\n",
    "out_file = open(f\"{name_of_model}/myfile.json\", \"w\")\n",
    "  \n",
    "json.dump(dict1, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_cv_model = AlexNet_2(18)\n",
    "checkpoint = torch.load(\"AlexNet_lr=0.0001_weight_decay=0_dropout=0.5/AlexNet_lr=0.0001_weight_decay=0_dropout=0.519.pth\")\n",
    "best_cv_model.load_state_dict(checkpoint['model'])\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_cv_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.drop(columns = 'Caption').join(df_train['Caption'].str.replace('\\\"', ''))\n",
    "df_test = df_test.drop(columns = 'Caption').join(df_test['Caption'].str.replace('\\\"', ''))\n",
    "whole_sentences = pd.concat([df_train['Caption'], df_test['Caption']], axis=0, ignore_index=True)\n",
    "whole_sentences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords as sw\n",
    "# stop_words = sw.words()\n",
    "STOPWORDS = set(sw.words('english'))\n",
    "\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_split(input_string): # string to pure word\n",
    "    splits = []\n",
    "    for sent in input_string:\n",
    "        # print(text)\n",
    "        sent = sent.lower() # lowercase\n",
    "        sent = re.sub(r'[^A-Za-z]+', ' ', sent) # remove symbols / digits\n",
    "        # text = re.sub(r'[0-9]','',text)\n",
    "        orig_sent = []\n",
    "        for item in sent.split():\n",
    "            if item not in STOPWORDS: # remove stopword\n",
    "                orig_sent.append(item)\n",
    "\n",
    "        lem = [lemmatizer.lemmatize(sent) for sent in orig_sent] # lammatisation\n",
    "\n",
    "        # token = [word_tokenize(word) for word in text_le]\n",
    "        splits.append(lem)\n",
    "    return splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_len = max(len(s) for s in sent_split(whole_sentences))\n",
    "max_seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "print(list(gensim.downloader.info()['models'].keys()))\n",
    "word_embedding_model = gensim.downloader.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "splitted_sentences = sent_split(whole_sentences)\n",
    "word_list = []\n",
    "for sent in splitted_sentences:\n",
    "    for word in sent:\n",
    "        word_list.append(word)\n",
    "len(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_list = []\n",
    "for sent in splitted_sentences:\n",
    "    for word in sent:\n",
    "        vocab_list.append(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Embedding lookup table\n",
    "import numpy as np\n",
    "emb_dim = word_embedding_model.vector_size\n",
    "word_set = set()\n",
    "min_freq = 5\n",
    "from collections import Counter\n",
    "c = Counter(vocab_list)\n",
    "for i in c:\n",
    "    if c[i] >= min_freq:\n",
    "        word_set.add(i)\n",
    "word_set.add('[PAD]')\n",
    "word_set.add('[UNKOWN]')\n",
    "word_list=list(word_set)\n",
    "word_list.sort()\n",
    "emb_table = []\n",
    "word_index = {}\n",
    "emb_table = []\n",
    "for i, word in enumerate(word_list):\n",
    "    word_index[word] = i\n",
    "    if word in word_embedding_model:\n",
    "        emb_table.append(word_embedding_model[word])\n",
    "    else:\n",
    "        emb_table.append([0]*emb_dim)\n",
    "emb_table = np.array(emb_table)\n",
    "    \n",
    "print(emb_table)\n",
    "emb_table.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    def tokenize(self, sentences, seq_len, word_index):\n",
    "        sentences = sent_split(sentences)\n",
    "        sent_encoded = []\n",
    "        for sent in sentences:\n",
    "            temp_encoded = [word_index[word] if word in word_index else word_index['[UNKOWN]'] for word in sent]\n",
    "            if len(temp_encoded) < seq_len:\n",
    "                temp_encoded += [word_index['[PAD]']] * (seq_len - len(temp_encoded))\n",
    "            else:\n",
    "                temp_encoded = temp_encoded[:seq_len]\n",
    "            sent_encoded.append(temp_encoded)\n",
    "        return sent_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = emb_table.shape[0]\n",
    "emb_dim = emb_table.shape[1]\n",
    "vocab_size, emb_dim"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "n_hidden = 300\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "seq_len = max_seq_len\n",
    "\n",
    "class Bi_LSTM_Emb(nn.Module):\n",
    "    def __init__(self,n_classes):\n",
    "        super(Bi_LSTM_Emb, self).__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "        # Initialize the Embedding layer with the lookup table we created \n",
    "        self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
    "        # Optional: set requires_grad = False to make this lookup table untrainable\n",
    "        self.emb.weight.requires_grad = False\n",
    "\n",
    "        self.lstm = nn.LSTM(emb_dim, n_hidden, batch_first =True, bidirectional=True)\n",
    "        self.linear = nn.Linear(n_hidden*2, n_classes)\n",
    "        self.sigm = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Get the embeded tensor\n",
    "        x = self.emb(x)        \n",
    "        # we will use the returned h_n of shape (num_layers * num_directions, batch, hidden_size): tensor containing the hidden state for t = seq_len.\n",
    "        # details of the outputs from nn.LSTM can be found from: https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html\n",
    "        lstm_out, (h_n,c_n) = self.lstm(x)\n",
    "        # concat the last hidden state from two direction\n",
    "        hidden_out = torch.cat((h_n[0,:,:],h_n[1,:,:]),1)\n",
    "        z = self.linear(hidden_out)\n",
    "        return self.sigm(z)\n",
    "\n",
    "    def calculate_f1_score_and_loss(self, loader, criterion, device):\n",
    "            self.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                y_pred = []\n",
    "                y_true = []\n",
    "                for data in loader:\n",
    "                    captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "                    captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "                    target = data['label'].to(device)\n",
    "                    outputs = self(captions)\n",
    "                    loss = criterion(outputs, target)\n",
    "                    val_loss += loss.item()\n",
    "                    outputs = outputs.cpu().numpy()\n",
    "                    argmax_indices = np.argmax(outputs, axis=1)\n",
    "                    outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "                    predicted = np.round(outputs)\n",
    "                    y_pred.extend(predicted)\n",
    "                    y_true.extend(target.cpu().numpy())\n",
    "                y_pred = np.array(y_pred)\n",
    "                y_true = np.array(y_true)\n",
    "                res = f1_score(y_true=y_true, y_pred=y_pred, average='samples')\n",
    "            return {'f1_score' : res, 'loss' : val_loss/len(loader)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "n_hidden = 300\n",
    "\n",
    "lr = 1e-4\n",
    "\n",
    "seq_len = max_seq_len\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "  def __init__(self, n_class):\n",
    "    super(LSTM, self).__init__()\n",
    "    self.emb = nn.Embedding(vocab_size, emb_dim)\n",
    "    self.emb.weight.data.copy_(torch.from_numpy(emb_table))\n",
    "    self.emb.weight.requires_grad = False\n",
    "    self.lstm = nn.LSTM(emb_dim, n_hidden, num_layers=2, batch_first =True, dropout=0.5)\n",
    "    self.linear = nn.Linear(n_hidden,n_class)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "  \n",
    "  def forward(self, input):\n",
    "    input = self.emb(input)\n",
    "    input,_ = self.lstm(input)\n",
    "    input = self.linear(input[:,-1,:])\n",
    "    return self.sigmoid(input)\n",
    "  \n",
    "  def calculate_f1_score_and_loss(self, loader, criterion, device):\n",
    "    self.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        y_pred = []\n",
    "        y_true = []\n",
    "        for data in loader:\n",
    "            captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "            captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "            target = data['label'].to(device)\n",
    "            outputs = self(captions)\n",
    "            loss = criterion(outputs, target)\n",
    "            val_loss += loss.item()\n",
    "            outputs = outputs.cpu().numpy()\n",
    "            argmax_indices = np.argmax(outputs, axis=1)\n",
    "            outputs[np.arange(outputs.shape[0]), argmax_indices] = 1\n",
    "            predicted = np.round(outputs)\n",
    "            y_pred.extend(predicted)\n",
    "            y_true.extend(target.cpu().numpy())\n",
    "        y_pred = np.array(y_pred)\n",
    "        y_true = np.array(y_true)\n",
    "        res = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "    return {'f1_score' : res, 'loss' : val_loss/len(loader)}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_nlp_model = Bi_LSTM_Emb(18)\n",
    "best_nlp_model = best_nlp_model.to(device)\n",
    "best_nlp_model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "optimizer = torch.optim.Adam(best_nlp_model.parameters(), lr=lr)\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_of_model = f'BiLSTM_lr={lr}_weight_decay={weight_decay}_dropout=0.5'\n",
    "os.mkdir(name_of_model)\n",
    "num_epochs = 3\n",
    "train_losses = torch.zeros(num_epochs)\n",
    "val_losses = torch.zeros(num_epochs)\n",
    "f1_scores = torch.zeros(num_epochs)\n",
    "for it in range(num_epochs):\n",
    "    train_loss = 0\n",
    "    for i, data in enumerate(train_loader):\n",
    "        best_nlp_model.train()\n",
    "        captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "        captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "        target = data['label'].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_nlp_model(captions)\n",
    "        loss = criterion(outputs, target)\n",
    "        train_loss+=loss.item()\n",
    "        if i % 500 == 0:\n",
    "            print(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if ((it % save_freq == 0) or (it == num_epochs - 1)) and it > 0:\n",
    "        checkpoint_path = f'{name_of_model}/{name_of_model}{it}.pth'\n",
    "        checkpoint = {'model': best_cv_model.state_dict(),\n",
    "                  'optimizer': optimizer.state_dict(),\n",
    "                  'epoch': num_epochs}\n",
    "        torch.save(checkpoint, checkpoint_path)        \n",
    "    print(it)\n",
    "    train_losses[it] = train_loss/len(train_loader)\n",
    "    result = best_nlp_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    val_losses[it] = result['loss']\n",
    "    f1_scores[it] = result['f1_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),valid_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.savefig(f'{name_of_model}/learning_curve.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict1 ={\n",
    "    \"f1_scores\": f1_scores.tolist(),\n",
    "    \"train_losses\": train_losses.tolist(),\n",
    "    \"valid_losses\": valid_losses.tolist()\n",
    "}\n",
    "  \n",
    "# the json file where the output must be stored\n",
    "out_file = open(f\"{name_of_model}/myfile.json\", \"w\")\n",
    "  \n",
    "json.dump(dict1, out_file, indent = 6)\n",
    "  \n",
    "out_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COMBINED MODEL"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to combine the NLP and classification model we need to concatenate the features and run a third classifier on this concatenated features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the third classifier"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to choose where to put either 19 or 18 labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = 18\n",
    "\n",
    "best_cv_model = Resnext50(num_classes=n_labels)\n",
    "best_nlp_model = Bi_LSTM_Emb(n_classes=n_labels)\n",
    "checkpoint = torch.load(\"ResNet4.pth\")\n",
    "# Load the model state dictionary from the checkpoint\n",
    "best_cv_model.load_state_dict(checkpoint['model'])\n",
    "\n",
    "checkpoint = torch.load(\"BiLSTM3.pth\")\n",
    "# Load the model state dictionary from the checkpoint\n",
    "best_nlp_model.load_state_dict(checkpoint['model'])\n",
    "# Define Classifier\n",
    "classifier = nn.Sequential(\n",
    "    nn.Linear(in_features=36, out_features=3100),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(in_features=3100, out_features=n_labels),\n",
    "    nn.Sigmoid()\n",
    ")\n",
    "\n",
    "\n",
    "# Switch model to GPU.\n",
    "best_cv_model = best_cv_model.to(device)\n",
    "best_nlp_model = best_nlp_model.to(device)\n",
    "classifier = classifier.to(device)\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss().to(device=device)\n",
    "optimizer = optim.Adam(classifier.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 20\n",
    "train_losses = torch.zeros(num_epochs)\n",
    "val_losses = torch.zeros(num_epochs)\n",
    "f1_scores = torch.zeros(num_epochs)\n",
    "iter = 0\n",
    "test_frequency = 100 \n",
    "saving_freq = 1\n",
    "for epoch in range(epochs):\n",
    "    classifier.train()\n",
    "    batch_loss_values = []\n",
    "    train_loss=0\n",
    "    for el in train_loader:\n",
    "        best_cv_model.train()\n",
    "        best_nlp_model.train()\n",
    "\n",
    "        targets = el['label'].to(device)\n",
    "        images = el['image'].to(device)\n",
    "       \n",
    "        captions = tokenizer.tokenize(el['caption'], seq_len, word_index)\n",
    "        captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        img_out = best_cv_model(images)\n",
    "        nlp_out = best_nlp_model(captions)\n",
    "        concatenating_outs = torch.concat((img_out, nlp_out), 1)\n",
    "\n",
    "        combined_model = classifier(concatenating_outs)\n",
    "        \n",
    "        #loss and backward pass\n",
    "        loss = criterion(combined_model, targets.type(torch.float))\n",
    "        train_loss += loss.item()\n",
    "        if i % 100 == 0:\n",
    "            print(loss.item())\n",
    "        # bl_value = loss.item()\n",
    "        # print(bl_value)\n",
    "\n",
    "        loss.backward()\n",
    "        \n",
    "\n",
    "        optimizer.step()\n",
    "        # optimizer.zero_grad() ### WHY??\n",
    "\n",
    "    #     # batch_loss_values.append(bl_value)\n",
    "    #     val_loss = 0\n",
    "    #     with torch.no_grad():\n",
    "    #     #     res = calculate_metrics(combined_model.cpu().numpy(), targets.cpu().numpy())\n",
    "    #     #     print(res['f1_score'])\n",
    "    #     # if iter % test_frequency == 0:\n",
    "    #     #     with torch.no_grad():\n",
    "    #     #         model_res = []\n",
    "    #     #         targets = []\n",
    "    #         y_pred = []\n",
    "    #         y_true = []\n",
    "    #         for data in valid_loader:\n",
    "    #             best_cv_model.eval()\n",
    "    #             best_nlp_model.eval()\n",
    "\n",
    "    #             # images part\n",
    "    #             targets = data['label'].to(device)\n",
    "    #             images = data['image'].to(device)\n",
    "\n",
    "    #             # NLP part\n",
    "    #             captions = tokenizer.tokenize(data['caption'], seq_len, word_index)\n",
    "    #             captions = torch.from_numpy(np.array(captions)).to(device)\n",
    "\n",
    "    #             # combining\n",
    "    #             img_out = best_cv_model(images)\n",
    "    #             nlp_out = best_nlp_model(captions)\n",
    "\n",
    "    #             combined_outs = classifier(torch.concat((img_out, nlp_out), 1))\n",
    "    #             loss_combined = criterion(combined_outs,targets)\n",
    "    #             val_loss += loss_combined.item()\n",
    "    #             combined_outs = combined_outs.cpu().numpy()\n",
    "    #             argmax_indices = np.argmax(combined_outs, axis=1)\n",
    "    #             combined_outs[np.arange(combined_outs.shape[0]), argmax_indices] = 1\n",
    "    #             predicted = np.round(combined_outs)\n",
    "    #             y_pred.extend(predicted)\n",
    "    #             y_true.extend(targets.cpu().numpy())\n",
    "    #         y_pred = np.array(y_pred)\n",
    "    #         y_true = np.array(y_true)\n",
    "    #         res = f1_score(y_true=y_true, y_pred=y_pred, average='micro')\n",
    "            \n",
    "    #     #  {'f1_score' : res, 'loss' : val_loss/len(valid_loader)}\n",
    "\n",
    "    print(f\"epoch {epoch}\")\n",
    "    # train_losses[it] = train_loss/len(train_loader)\n",
    "    # # result = best_nlp_model.calculate_f1_score_and_loss(valid_loader, criterion=criterion, device=device)\n",
    "    # val_losses[it] = val_loss/len(valid_loader)\n",
    "    # f1_scores[it] = res\n",
    "    # print(\"epoch:{:2d} iter:{:3d} train: loss:{:.3f}\".format(epoch, iter, loss_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.arange(num_epochs),train_losses, label = \"training loss\")\n",
    "plt.plot(np.arange(num_epochs),val_losses,label = \"validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
